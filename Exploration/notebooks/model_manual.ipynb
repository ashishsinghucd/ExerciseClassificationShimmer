{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29134f1f-9946-4dd3-8f22-72548cde45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import math\n",
    "import pandas as pd\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e83c85-451f-4d6d-b0a5-d852232f3d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716b3c27-4ae5-48f4-9e68-4266aee2952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ef6d08-96cf-4b37-8217-8749770f1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d549da-a2a4-4ba9-bc6d-47d665c769b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fbc2108-6d98-404d-a0dd-a633beafc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd410b2-8f1c-4f5f-8faa-96b79816a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/home/ashish/Results/Datasets/Shimmer/MP/Catch22/{}/\"\n",
    "data_type = \"default\"\n",
    "SEED_VALUES = [103007, 1899797, 191099]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4d0a25c-a126-45f3-81a7-5caed7a16bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_clf = [\n",
    "            RidgeClassifier(), \n",
    "            LogisticRegression(C=0.001, max_iter=150),\n",
    "            RandomForestClassifier(),           \n",
    "            GaussianNB(), \n",
    "            KNeighborsClassifier(n_neighbors=40),\n",
    "            #ExtraTreesClassifier(n_estimators=100)\n",
    "            svm.SVC(kernel=\"linear\", C=0.025), \n",
    "            ]\n",
    "\n",
    "\n",
    "name_clf = [\n",
    "            \"RidgeClassifier\",\n",
    "            \"LogisticRegression\",\n",
    "            \"RandomForestClassifier\",       \n",
    "            \"GaussianNB\",\n",
    "            \"KNeighborsClassifier\",\n",
    "            #ExtraTreesClassifier(n_estimators=100)\n",
    "            \"svm.SVC\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b11b69c-e7e4-4b64-a2d7-ad0c9c599ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1426, 990) (595, 990) (1426,) (595,)\n"
     ]
    }
   ],
   "source": [
    "sv = 103007\n",
    "output_path = \"/home/ashish/Results/Datasets/Shimmer/MP/Catch22/{}/\"\n",
    "data_type = \"default\"\n",
    "\n",
    "\n",
    "train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n",
    "test_X = pd.read_csv(output_path.format(sv) + \"x_test_{}.csv\".format(data_type))\n",
    "\n",
    "train_Y = np.load(os.path.join(output_path.format(sv), \"y_train_{}.csv.npy\".format(data_type)), allow_pickle=True)\n",
    "test_Y = np.load(os.path.join(output_path.format(sv), \"y_test_{}.npy\".format(data_type)), allow_pickle=True)\n",
    "    # train_df = shuffle(train_df)\n",
    "print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4954d9-4873-4d63-8d2f-04c252337357",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 600, num = 8)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               # 'bootstrap': bootstrap\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a7373d9-ee75-4c0c-9959-aa405db4b40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f53665f3-422b-42ad-a5b4-e55a42947be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1426, 990) (595, 990) (1426,) (595,)\n",
      "(1426, 263) (595, 263) (1426,) (595,)\n",
      "--------------1e-05-----------\n",
      "0.85 accuracy with a standard deviation of 0.04\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/home/ashish/Results/Datasets/Shimmer/MP/Catch22/{}/\"\n",
    "data_type = \"default\"\n",
    "sv = 103007\n",
    "\n",
    "\n",
    "train_pid = np.load(output_path.format(sv) + \"TRAIN_{}_pid.npy\".format(data_type), allow_pickle=True)\n",
    "\n",
    "temp_pid = train_pid[:, 0]\n",
    "temp_pid = [i.split(\"_\")[0] for i in temp_pid]\n",
    "groups = {\"g1\":[], \"g2\":[], \"g3\":[], \"g4\":[]}\n",
    "unique_pid = list(np.unique(temp_pid))\n",
    "groups[\"g1\"].extend(unique_pid[0:9])\n",
    "groups[\"g2\"].extend(unique_pid[9:18])\n",
    "groups[\"g3\"].extend(unique_pid[18:27])\n",
    "groups[\"g4\"].extend(unique_pid[27:])\n",
    "\n",
    "\n",
    "groups_list = []\n",
    "\n",
    "for i in temp_pid:\n",
    "    if i in groups[\"g1\"]:\n",
    "        groups_list.append(1)\n",
    "    elif i in groups[\"g2\"]:\n",
    "        groups_list.append(2)\n",
    "    elif i in groups[\"g3\"]:\n",
    "        groups_list.append(3)\n",
    "    else:\n",
    "        groups_list.append(4)\n",
    "        \n",
    "        \n",
    "\n",
    "# for i, model in enumerate(list_clf):\n",
    "# print(\"--------------------------- {}----------------\".format(name_clf[i]))\n",
    "avg_acc = []\n",
    "# for sv in SEED_VALUES:\n",
    "train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n",
    "test_X = pd.read_csv(output_path.format(sv) + \"x_test_{}.csv\".format(data_type))\n",
    "\n",
    "train_Y = np.load(os.path.join(output_path.format(sv), \"y_train_{}.csv.npy\".format(data_type)), allow_pickle=True)\n",
    "test_Y = np.load(os.path.join(output_path.format(sv), \"y_test_{}.npy\".format(data_type)), allow_pickle=True)\n",
    "print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_X = sc.fit_transform(train_X)\n",
    "test_X = sc.transform(test_X)\n",
    "\n",
    "#  ExtraTreesClassifier(n_estimators=50) LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_Y)\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_Y)\n",
    "model_features = SelectFromModel(lsvc, prefit=True)\n",
    "train_X = model_features.transform(train_X)\n",
    "test_X = model_features.transform(test_X)\n",
    "\n",
    "# pca = PCA(n_components=700).fit(train_X)\n",
    "# pca = TruncatedSVD(n_components=800, n_iter=7, random_state=42).fit(train_X)\n",
    "# VarianceThreshold(threshold=(.6 * (1 - .7))).fit(train_X)\n",
    "# \n",
    "# FactorAnalysis(n_components=600, random_state=0).fit(train_X)\n",
    "# pca = SelectKBest(mutual_info_classif, k=600).fit(train_X, train_Y)\n",
    "# train_X = pca.transform(train_X)\n",
    "# test_X = pca.transform(test_X)\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# train_X = sc.fit_transform(train_X)\n",
    "# test_X = sc.transform(test_X)\n",
    "\n",
    "print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "\n",
    "# model = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, \n",
    "#                                random_state=42, n_jobs = -1)\n",
    "\n",
    "# lm = [100000, 10000, 1000, 1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=4)\n",
    "# for p in lm:\n",
    "# model =  LogisticRegression(C=0.01, max_iter=400) \n",
    "model =  RidgeClassifier(alpha=300) \n",
    "\n",
    "scores = cross_val_score(model, train_X, train_Y, cv=group_kfold, groups=groups_list, scoring=\"accuracy\")\n",
    "print(\"--------------{}-----------\".format(p))\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "# model.fit(train_X, train_Y)\n",
    "# pred_Y = model.predict(test_X)\n",
    "# accuracy = metrics.accuracy_score(test_Y, pred_Y)\n",
    "# print(accuracy)\n",
    "    # model.fit(train_X, train_Y)\n",
    "# pred_Y = model.predict(test_X)\n",
    "\n",
    "# regressor = RandomForestClassifier(n_estimators=200, max_depth=50)\n",
    "# model = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid, n_iter = 7, cv = 3, verbose=2, \n",
    "#                            random_state=42, n_jobs = -1)\n",
    "# svm.SVC(kernel='linear', C=0.001)  LogisticRegression(C=0.001, max_iter=150)   KNeighborsClassifier(n_neighbors=40) \n",
    "# model =   RidgeClassifier(alpha=1000) #LogisticRegression(C=0.001, max_iter=150)\n",
    "# model.fit(train_X, train_Y)\n",
    "# pred_Y = model.predict(test_X)\n",
    "\n",
    "\n",
    "# accuracy = metrics.accuracy_score(test_Y, pred_Y)\n",
    "# confusion_matrix = metrics.confusion_matrix(test_Y, pred_Y)\n",
    "# classification_report = metrics.classification_report(test_Y, pred_Y)\n",
    "\n",
    "# print(accuracy) #, print(confusion_matrix), print(classification_report)\n",
    "# avg_acc.append(accuracy)\n",
    "\n",
    "# print(round(np.mean(avg_acc),2), np.std(avg_acc))\n",
    "\n",
    "\n",
    "# LR 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d2a4bb-9594-47bc-9c67-03a8d57653a6",
   "metadata": {},
   "source": [
    "0.76, \n",
    "\n",
    "\n",
    "0.77,  LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_Y) is the best setting\n",
    "\n",
    "300 semms best alpha for RidgeRegression rowing\n",
    "\n",
    "400 for mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f58a17de-c5c8-4a7d-87f2-b9245fd7527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1773, 990) (760, 990) (1773,) (760,)\n",
      "(1773, 346) (760, 346) (1773,) (760,)\n",
      "0.7657894736842106\n",
      "(1803, 990) (730, 990) (1803,) (730,)\n",
      "(1803, 333) (730, 333) (1803,) (730,)\n",
      "0.7082191780821918\n",
      "(1773, 990) (760, 990) (1773,) (760,)\n",
      "(1773, 347) (760, 347) (1773,) (760,)\n",
      "0.7513157894736842\n",
      "0.74 0.024452094624567374\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/home/ashish/Results/Datasets/Shimmer/Rowing/Catch22/{}/\"\n",
    "data_type = \"default\"\n",
    "\n",
    "\n",
    "# for i, model in enumerate(list_clf):\n",
    "# print(\"--------------------------- {}----------------\".format(name_clf[i]))\n",
    "avg_acc = []\n",
    "for sv in SEED_VALUES:\n",
    "    train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n",
    "    test_X = pd.read_csv(output_path.format(sv) + \"x_test_{}.csv\".format(data_type))\n",
    "    \n",
    "    train_Y = np.load(os.path.join(output_path.format(sv), \"y_train_{}.csv.npy\".format(data_type)), allow_pickle=True)\n",
    "    test_Y = np.load(os.path.join(output_path.format(sv), \"y_test_{}.npy\".format(data_type)), allow_pickle=True)\n",
    "    print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "    \n",
    "\n",
    "    sc = StandardScaler()\n",
    "    train_X = sc.fit_transform(train_X)\n",
    "    test_X = sc.transform(test_X)\n",
    "    \n",
    "    lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_Y)\n",
    "    model_features = SelectFromModel(lsvc, prefit=True)\n",
    "    train_X = model_features.transform(train_X)\n",
    "    test_X = model_features.transform(test_X)\n",
    "    \n",
    "    print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "\n",
    "    # regressor = RandomForestClassifier()\n",
    "    # model = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid, n_iter = 7, cv = 3, verbose=2, \n",
    "    #                            random_state=42, n_jobs = -1)\n",
    "    # svm.SVC(kernel='linear', C=0.001)  LogisticRegression(C=0.001, max_iter=150)   KNeighborsClassifier(n_neighbors=40) \n",
    "    # model = LogisticRegression(C=1, max_iter=500)#LogisticRegression(C=0.001, max_iter=150)\n",
    "    model =  RidgeClassifier(alpha=300)  #RidgeClassifier(alpha=400) \n",
    "\n",
    "    model.fit(train_X, train_Y)\n",
    "    pred_Y = model.predict(test_X)\n",
    "\n",
    "\n",
    "    accuracy = metrics.accuracy_score(test_Y, pred_Y)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_Y, pred_Y)\n",
    "    classification_report = metrics.classification_report(test_Y, pred_Y)\n",
    "\n",
    "    print(accuracy) #, print(confusion_matrix), print(classification_report)\n",
    "    avg_acc.append(accuracy)\n",
    "\n",
    "print(round(np.mean(avg_acc),2), np.std(avg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88783fd1-897b-44ae-b9d3-6eae37c60c82",
   "metadata": {},
   "source": [
    "# MP 990\n",
    "\n",
    "- LogisticRegression 0.84 0.015\n",
    "- RandomForestClassifier 0.81 0.02\n",
    "- svm.SVC(kernel='linear', C=0.001) 0.82 0.01\n",
    "- RidgeClassifier(alpha=1000) 0.83 0.031\n",
    "\n",
    "\n",
    "# Rowing 990\n",
    "\n",
    "- LogisticRegression 0.75 0.015\n",
    "- RandomForestClassifier 0.72 0.025\n",
    "- svm.SVC(kernel='linear', C=0.001) 0.74 0.01\n",
    "- RidgeClassifier(alpha=1000) 0.74 0.0244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a612fbc2-0d15-40b0-b959-420da3278b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1426, 15893) (595, 15893) (1426,) (595,)\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/home/ashish/Results/Datasets/Shimmer/MP/tsfresh/{}/\"\n",
    "data_type = \"default\"\n",
    "sv = 103007\n",
    "\n",
    "train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n",
    "test_X = pd.read_csv(output_path.format(sv) + \"x_test_{}.csv\".format(data_type))\n",
    "\n",
    "train_Y = np.load(output_path.format(sv) + \"y_train_{}.npy\".format(data_type), allow_pickle=True)\n",
    "test_Y = np.load(output_path.format(sv) + \"y_test_{}.npy\".format(data_type), allow_pickle=True)\n",
    "\n",
    "print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b9eae19b-ad80-4e3c-9ea2-9355068dc435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1426, 15893) (595, 15893) (1426,) (595,)\n",
      "(1426, 540) (595, 540) (1426,) (595,)\n",
      "0.94 accuracy with a standard deviation of 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashish/VirtualEnvironments/mlv1/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/home/ashish/Results/Datasets/Shimmer/MP/tsfresh/{}/\"\n",
    "data_type = \"default\"\n",
    "\n",
    "\n",
    "sv = 103007\n",
    "train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n",
    "test_X = pd.read_csv(output_path.format(sv) + \"x_test_{}.csv\".format(data_type))\n",
    "\n",
    "train_Y = np.load(output_path.format(sv) + \"y_train_{}.npy\".format(data_type), allow_pickle=True)\n",
    "test_Y = np.load(output_path.format(sv) + \"y_test_{}.npy\".format(data_type), allow_pickle=True)\n",
    "\n",
    "train_pid = np.load(output_path.format(sv) + \"TRAIN_{}_pid.npy\".format(data_type), allow_pickle=True)\n",
    "test_pid = np.load(output_path.format(sv) + \"TEST_{}_pid.npy\".format(data_type), allow_pickle=True)\n",
    "\n",
    "print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "\n",
    "\n",
    "temp_pid = train_pid[:, 0]\n",
    "temp_pid = [i.split(\"_\")[0] for i in temp_pid]\n",
    "groups = {\"g1\":[], \"g2\":[], \"g3\":[], \"g4\":[]}\n",
    "unique_pid = list(np.unique(temp_pid))\n",
    "groups[\"g1\"].extend(unique_pid[0:9])\n",
    "groups[\"g2\"].extend(unique_pid[9:18])\n",
    "groups[\"g3\"].extend(unique_pid[18:27])\n",
    "groups[\"g4\"].extend(unique_pid[27:])\n",
    "\n",
    "\n",
    "groups_list = []\n",
    "\n",
    "for i in temp_pid:\n",
    "    if i in groups[\"g1\"]:\n",
    "        groups_list.append(1)\n",
    "    elif i in groups[\"g2\"]:\n",
    "        groups_list.append(2)\n",
    "    elif i in groups[\"g3\"]:\n",
    "        groups_list.append(3)\n",
    "    else:\n",
    "        groups_list.append(4)\n",
    "\n",
    "        \n",
    "    \n",
    "sc = StandardScaler()\n",
    "train_X = sc.fit_transform(train_X)\n",
    "test_X = sc.transform(test_X)\n",
    "\n",
    "\n",
    "#  ExtraTreesClassifier(n_estimators=50) LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_Y)\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_Y)\n",
    "model_features = SelectFromModel(lsvc, prefit=True)\n",
    "train_X = model_features.transform(train_X)\n",
    "test_X = model_features.transform(test_X)\n",
    "\n",
    "# pca = PCA(n_components=1000).fit(train_X)\n",
    "# pca = TruncatedSVD(n_components=800, n_iter=7, random_state=42).fit(train_X)\n",
    "# VarianceThreshold(threshold=(.6 * (1 - .7))).fit(train_X)\n",
    "# \n",
    "# FactorAnalysis(n_components=600, random_state=0).fit(train_X)\n",
    "# pca = SelectKBest(mutual_info_classif, k=600).fit(train_X, train_Y)\n",
    "# train_X = pca.transform(train_X)\n",
    "# test_X = pca.transform(test_X)\n",
    "\n",
    "lm = [100000, 10000, 1000, 1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "\n",
    "\n",
    "# regressor = RandomForestClassifier()\n",
    "# model = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid, n_iter = 7, cv = 3, verbose=2, \n",
    "#                            random_state=42, n_jobs = -1)\n",
    "# svm.SVC(kernel='linear', C=0.001)  LogisticRegression(C=0.001, max_iter=150)   KNeighborsClassifier(n_neighbors=40) \n",
    "\n",
    "group_kfold = GroupKFold(n_splits=4)\n",
    "\n",
    "# for l in lm:\n",
    "model =  RidgeClassifier(alpha=400) # LogisticRegression(C=0.01, max_iter=500,) #LogisticRegression(C=0.001, max_iter=150)\n",
    "scores = cross_val_score(model, train_X, train_Y, cv=group_kfold, groups=groups_list, scoring=\"accuracy\")\n",
    "# print(\"--------------{}-----------\".format(l))\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "\n",
    "# model.fit(train_X, train_Y)\n",
    "# pred_Y = model.predict(test_X)\n",
    "\n",
    "\n",
    "# accuracy = metrics.accuracy_score(test_Y, pred_Y)\n",
    "# confusion_matrix = metrics.confusion_matrix(test_Y, pred_Y)\n",
    "# classification_report = metrics.classification_report(test_Y, pred_Y)\n",
    "\n",
    "# print(accuracy) #, print(confusion_matrix), print(classification_report)\n",
    "\n",
    "\n",
    "# mp 1500 in linearsvc\n",
    "# rowing default\n",
    "# 0.01 for both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77b8aa55-7a4b-401f-b6f8-3732dd7b7032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1773, 16331) (760, 16331) (1773,) (760,)\n",
      "--------------100000-----------\n",
      "0.75 accuracy with a standard deviation of 0.07\n",
      "--------------10000-----------\n",
      "0.75 accuracy with a standard deviation of 0.07\n",
      "--------------1000-----------\n",
      "0.75 accuracy with a standard deviation of 0.07\n",
      "--------------1-----------\n",
      "0.77 accuracy with a standard deviation of 0.04\n",
      "--------------0.01-----------\n",
      "0.76 accuracy with a standard deviation of 0.04\n",
      "--------------0.001-----------\n",
      "0.76 accuracy with a standard deviation of 0.04\n",
      "--------------0.0001-----------\n",
      "0.74 accuracy with a standard deviation of 0.04\n",
      "--------------1e-05-----------\n",
      "0.67 accuracy with a standard deviation of 0.04\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/home/ashish/Results/Datasets/Shimmer/Rowing/tsfresh/{}/\"\n",
    "data_type = \"default\"\n",
    "\n",
    "\n",
    "sv = 103007\n",
    "train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n",
    "test_X = pd.read_csv(output_path.format(sv) + \"x_test_{}.csv\".format(data_type))\n",
    "\n",
    "train_Y = np.load(output_path.format(sv) + \"y_train_{}.npy\".format(data_type), allow_pickle=True)\n",
    "test_Y = np.load(output_path.format(sv) + \"y_test_{}.npy\".format(data_type), allow_pickle=True)\n",
    "\n",
    "train_pid = np.load(output_path.format(sv) + \"TRAIN_{}_pid.npy\".format(data_type), allow_pickle=True)\n",
    "\n",
    "test_pid = np.load(output_path.format(sv) + \"TEST_{}_pid.npy\".format(data_type), allow_pickle=True)\n",
    "\n",
    "print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_X = sc.fit_transform(train_X)\n",
    "test_X = sc.transform(test_X)\n",
    "\n",
    "lm = [100000, 10000, 1000, 1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "\n",
    "# regressor = RandomForestClassifier()\n",
    "# model = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid, n_iter = 7, cv = 3, verbose=2, \n",
    "#                            random_state=42, n_jobs = -1)\n",
    "# svm.SVC(kernel='linear', C=0.001)  LogisticRegression(C=0.001, max_iter=150)   KNeighborsClassifier(n_neighbors=40) \n",
    "\n",
    "group_kfold = GroupKFold(n_splits=4)\n",
    "\n",
    "# for l in lm:\n",
    "#     model =  LogisticRegression(C=l, max_iter=500,) #LogisticRegression(C=0.001, max_iter=150)\n",
    "#     scores = cross_val_score(model, train_X, train_Y, cv=4, groups=groups_list, scoring=\"accuracy\")\n",
    "#     print(\"--------------{}-----------\".format(l))\n",
    "#     print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "for l in lm:\n",
    "    scores = []\n",
    "    model =  LogisticRegression(C=l, max_iter=500,)\n",
    "    for train_index, test_index in group_kfold.split(train_X, train_Y, groups_list):\n",
    "        X_train, X_test = train_X[train_index], train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        pred_Y = model.predict(X_test)\n",
    "        accuracy = metrics.accuracy_score(y_test, pred_Y)\n",
    "        scores.append(accuracy)\n",
    "    print(\"--------------{}-----------\".format(l))\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "# model.fit(train_X, train_Y)\n",
    "# pred_Y = model.predict(test_X)\n",
    "\n",
    "\n",
    "# accuracy = metrics.accuracy_score(test_Y, pred_Y)\n",
    "# confusion_matrix = metrics.confusion_matrix(test_Y, pred_Y)\n",
    "# classification_report = metrics.classification_report(test_Y, pred_Y)\n",
    "\n",
    "# print(accuracy) #, print(confusion_matrix), print(classification_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "536ab3fd-777f-48d6-9079-1912e3dfd0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8283433133732535,\n",
       " 0.7060185185185185,\n",
       " 0.7571428571428571,\n",
       " 0.7761904761904762]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4753ea17-89b7-4033-bfd6-eff532e47aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1773"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe052830-f334-48ec-8831-279ede140701",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pid = train_pid[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7eb3e638-e778-46a1-b488-a7e7382ddfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pid = [i.split(\"_\")[0] for i in temp_pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8887009b-4180-40b0-b792-ae5ec80227bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {\"g1\":[], \"g2\":[], \"g3\":[], \"g4\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37c92d1d-af33-481c-81e9-5b37c0fe0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pid = train_pid[:, 0]\n",
    "temp_pid = [i.split(\"_\")[0] for i in temp_pid]\n",
    "groups = {\"g1\":[], \"g2\":[], \"g3\":[], \"g4\":[]}\n",
    "unique_pid = list(np.unique(temp_pid))\n",
    "groups[\"g1\"].extend(unique_pid[0:9])\n",
    "groups[\"g2\"].extend(unique_pid[9:18])\n",
    "groups[\"g3\"].extend(unique_pid[18:27])\n",
    "groups[\"g4\"].extend(unique_pid[27:])\n",
    "\n",
    "\n",
    "groups_list = []\n",
    "\n",
    "for i in temp_pid:\n",
    "    if i in groups[\"g1\"]:\n",
    "        groups_list.append(1)\n",
    "    elif i in groups[\"g2\"]:\n",
    "        groups_list.append(2)\n",
    "    elif i in groups[\"g3\"]:\n",
    "        groups_list.append(3)\n",
    "    else:\n",
    "        groups_list.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd385466-1c76-4561-a85b-18767a29e069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P10', 'P11', 'P12', 'P13', 'P15', 'P16', 'P17', 'P18', 'P2']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10954ce2-73ac-4b0d-970d-441e94f8450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_list = []\n",
    "\n",
    "for i in temp_pid:\n",
    "    if i in groups[\"g1\"]:\n",
    "        groups_list.append(1)\n",
    "    elif i in groups[\"g2\"]:\n",
    "        groups_list.append(2)\n",
    "    elif i in groups[\"g3\"]:\n",
    "        groups_list.append(3)\n",
    "    else:\n",
    "        groups_list.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "98612c85-6dda-43b3-adf4-abcc4950d9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1773, 16331) (760, 16331) (1773,) (760,)\n",
      "(1773, 809) (760, 809) (1773,) (760,)\n",
      "0.8144736842105263\n",
      "(1803, 16331) (730, 16331) (1803,) (730,)\n",
      "(1803, 749) (730, 749) (1803,) (730,)\n",
      "0.7780821917808219\n",
      "(1773, 16331) (760, 16331) (1773,) (760,)\n",
      "(1773, 785) (760, 785) (1773,) (760,)\n",
      "0.8197368421052632\n",
      "0.8 0.018520713466991606\n"
     ]
    }
   ],
   "source": [
    "output_path = \"/home/ashish/Results/Datasets/Shimmer/Rowing/tsfresh/{}/\"\n",
    "data_type = \"default\"\n",
    "\n",
    "\n",
    "# for i, model in enumerate(list_clf):\n",
    "# print(\"--------------------------- {}----------------\".format(name_clf[i]))\n",
    "avg_acc = []\n",
    "for sv in SEED_VALUES:\n",
    "    train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n",
    "    test_X = pd.read_csv(output_path.format(sv) + \"x_test_{}.csv\".format(data_type))\n",
    "\n",
    "    train_Y = np.load(output_path.format(sv) + \"y_train_{}.npy\".format(data_type), allow_pickle=True)\n",
    "    test_Y = np.load(output_path.format(sv) + \"y_test_{}.npy\".format(data_type), allow_pickle=True)\n",
    "    print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "    \n",
    "\n",
    "    sc = StandardScaler()\n",
    "    train_X = sc.fit_transform(train_X)\n",
    "    test_X = sc.transform(test_X)\n",
    "\n",
    "    lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False,  max_iter=1500).fit(train_X, train_Y)\n",
    "    model_features = SelectFromModel(lsvc, prefit=True)\n",
    "    train_X = model_features.transform(train_X)\n",
    "    test_X = model_features.transform(test_X)\n",
    "    print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "\n",
    "\n",
    "    # regressor = RandomForestClassifier()\n",
    "    # model = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid, n_iter = 7, cv = 3, verbose=2, \n",
    "    #                            random_state=42, n_jobs = -1)\n",
    "    # svm.SVC(kernel='linear', C=0.001)  LogisticRegression(C=0.001, max_iter=150)   KNeighborsClassifier(n_neighbors=40) \n",
    "    model = LogisticRegression(C=0.01, max_iter=500) #LogisticRegression(C=0.01, max_iter=500)#LogisticRegression(C=0.001, max_iter=150)\n",
    "    model.fit(train_X, train_Y)\n",
    "    pred_Y = model.predict(test_X)\n",
    "\n",
    "\n",
    "    accuracy = metrics.accuracy_score(test_Y, pred_Y)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_Y, pred_Y)\n",
    "    classification_report = metrics.classification_report(test_Y, pred_Y)\n",
    "\n",
    "    print(accuracy) #, print(confusion_matrix), print(classification_report)\n",
    "    avg_acc.append(accuracy)\n",
    "\n",
    "print(round(np.mean(avg_acc),2), np.std(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581f250-3e82-4e81-be20-ec3eb1d5e9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "627c160a-d84c-41aa-b9f3-ebd0ebf0b4bd",
   "metadata": {},
   "source": [
    "# MP 15893 features\n",
    "\n",
    "- LogisticRegression 0.88 0.02\n",
    "- RandomForestClassifier 0.83 0.016\n",
    "- svm.SVC(kernel='linear', C=0.001) 0.89 0.027\n",
    "- RidgeClassifier(alpha=300) 0.87 0.015\n",
    "\n",
    "\n",
    "# Rowing 16331\n",
    "\n",
    "- LogisticRegression 0.8 0.01\n",
    "- RandomForestClassifier 0.73 0.02\n",
    "- svm.SVC(kernel='linear', C=0.001) 0.77 0.014\n",
    "- RidgeClassifier(alpha=300) 0.79 0.024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6888b4e-9f61-4811-9a66-c0d4e8044f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cross_val_splits(pid, full_df):\n",
    "    second_last_col = full_df.columns[-2]\n",
    "    pid_excl_df = full_df[full_df[second_last_col]!=pid]\n",
    "    pid_incl_df = full_df[full_df[second_last_col]==pid]\n",
    "        \n",
    "    train_X, train_Y = pid_excl_df.iloc[:, :-2], pid_excl_df.iloc[:, -1]\n",
    "    test_X, test_Y = pid_incl_df.iloc[:, :-2], pid_incl_df.iloc[:, -1]\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "55aedfed-eb5a-4a71-8bd1-e00c872c575e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2483,), (1773, 991))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363167ee-3011-4293-8bbd-1d7edff2a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/home/ashish/Results/Datasets/Shimmer/Rowing/tsfresh/{}/\"\n",
    "data_type = \"default\"\n",
    "\n",
    "\n",
    "sv = 103007\n",
    "train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n",
    "test_X = pd.read_csv(output_path.format(sv) + \"x_test_{}.csv\".format(data_type))\n",
    "\n",
    "train_Y = np.load(os.path.join(output_path.format(sv), \"y_train_{}.npy\".format(data_type)), allow_pickle=True)\n",
    "test_Y = np.load(os.path.join(output_path.format(sv), \"y_test_{}.npy\".format(data_type)), allow_pickle=True)\n",
    "\n",
    "train_pid = np.load(output_path.format(sv) + \"TRAIN_{}_pid.npy\".format(data_type), allow_pickle=True)\n",
    "test_pid = np.load(output_path.format(sv) + \"TEST_{}_pid.npy\".format(data_type), allow_pickle=True)\n",
    "\n",
    "print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "\n",
    "\n",
    "# lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_Y)\n",
    "# model_features = SelectFromModel(lsvc, prefit=True)\n",
    "# train_X = model_features.transform(train_X)\n",
    "# test_X = model_features.transform(test_X)\n",
    "\n",
    "\n",
    "temp_pid = train_pid[:, 0]\n",
    "temp_pid = [i.split(\"_\")[0] for i in temp_pid]\n",
    "train_df = pd.DataFrame(train_X)\n",
    "train_df[\"pid\"] = temp_pid\n",
    "train_df[\"label\"] = train_Y\n",
    "temp_pid = test_pid[:, 0]\n",
    "temp_pid = [i.split(\"_\")[0] for i in temp_pid]\n",
    "test_df = pd.DataFrame(test_X)\n",
    "test_df[\"pid\"] = temp_pid\n",
    "test_df[\"label\"] = test_Y\n",
    "\n",
    "\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "# full_df = shuffle(full_df)\n",
    "\n",
    "model = LogisticRegression(C=0.01, max_iter=500)\n",
    "\n",
    "unique_pids = list(np.unique(full_df.loc[:, \"pid\"]))\n",
    "\n",
    "scores = []\n",
    "accuracy_scores_mapping = {\"pid\": [], \"score\": []}\n",
    "\n",
    "for pid in unique_pids:\n",
    "    logger.info(\"The pid: {} \".format(pid))\n",
    "    accuracy_scores_mapping[\"pid\"].append(pid)\n",
    "    train_X, train_Y, test_X, test_Y = generate_cross_val_splits(pid, full_df)\n",
    "    logger.info(\"Training shape: {} {}\".format(train_X.shape, train_Y.shape))\n",
    "    logger.info(\"Testing shape: {} {}\".format(test_X.shape, test_Y.shape))\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    train_X = sc.fit_transform(train_X)\n",
    "    test_X = sc.transform(test_X)\n",
    "    \n",
    "    lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False,  max_iter=1500).fit(train_X, train_Y)\n",
    "    model_features = SelectFromModel(lsvc, prefit=True)\n",
    "    train_X = model_features.transform(train_X)\n",
    "    test_X = model_features.transform(test_X)\n",
    "\n",
    "    model.fit(train_X, train_Y)\n",
    "\n",
    "    pred_Y = model.predict(test_X)\n",
    "    accuracy = metrics.accuracy_score(test_Y, pred_Y)\n",
    "    \n",
    "    scores.append(accuracy)\n",
    "    accuracy_scores_mapping[\"score\"].append(accuracy)\n",
    "    logger.info(\"The accuracy: {} \".format(accuracy))\n",
    "accuracy_scores_mapping_df = pd.DataFrame(accuracy_scores_mapping)\n",
    "accuracy_scores_mapping_df.to_csv(\"/tmp/accuracy_cross_val_shimmer_mp_manual.csv\", index=False)\n",
    "logger.info(\"The average accuracy: {} std: {}\".format(np.mean(scores), np.std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c39879-0b56-4055-b39a-1b997f3f235c",
   "metadata": {},
   "source": [
    "Rowing  0.7377210770348024 std: 0.157, catch 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2805a5-feed-440a-94f4-c12d7b0dcab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1773, 5634) (760, 5634) (1773,) (760,)\n",
      "0.7921052631578948\n",
      "(1803, 5634) (730, 5634) (1803,) (730,)\n",
      "0.736986301369863\n",
      "(1773, 5634) (760, 5634) (1773,) (760,)\n",
      "0.7736842105263158\n",
      "0.77 0.02291086997014777\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_path = \"/home/ashish/Results/Datasets/HPE3/Rowing/tsfresh/{}/\"\n",
    "data_type = \"default\"\n",
    "\n",
    "\n",
    "# for i, model in enumerate(list_clf):\n",
    "# print(\"--------------------------- {}----------------\".format(name_clf[i]))\n",
    "avg_acc = []\n",
    "for sv in SEED_VALUES:\n",
    "    train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n",
    "    test_X = pd.read_csv(output_path.format(sv) + \"x_test_{}.csv\".format(data_type))\n",
    "\n",
    "    # train_Y = pd.read_csv(output_path.format(sv) + \"y_train_{}.csv\".format(data_type))\n",
    "    # test_Y = pd.read_csv(output_path.format(sv) + \"y_test_{}.csv\".format(data_type))\n",
    "    train_Y = np.load(os.path.join(output_path.format(sv), \"y_train_{}.npy\".format(data_type)), allow_pickle=True)\n",
    "    test_Y = np.load(os.path.join(output_path.format(sv), \"y_test_{}.npy\".format(data_type)), allow_pickle=True)\n",
    "    print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "    \n",
    "\n",
    "    sc = StandardScaler()\n",
    "    train_X = sc.fit_transform(train_X)\n",
    "    test_X = sc.transform(test_X)\n",
    "\n",
    "    # lsvc = LinearSVC(C=0.001, penalty=\"l1\", dual=False,  max_iter=1500).fit(train_X, train_Y)\n",
    "    # model_features = SelectFromModel(lsvc, prefit=True)\n",
    "    # train_X = model_features.transform(train_X)\n",
    "    # test_X = model_features.transform(test_X)\n",
    "    # print(train_X.shape, test_X.shape, train_Y.shape, test_Y.shape)\n",
    "\n",
    "\n",
    "    # regressor = RandomForestClassifier()\n",
    "    # model = RandomizedSearchCV(estimator = regressor, param_distributions = random_grid, n_iter = 7, cv = 3, verbose=2, \n",
    "    #                            random_state=42, n_jobs = -1)\n",
    "    # svm.SVC(kernel='linear', C=0.001)  LogisticRegression(C=0.001, max_iter=150)   KNeighborsClassifier(n_neighbors=40) \n",
    "    model = LogisticRegression(C=0.01, max_iter=500) #LogisticRegression(C=0.01, max_iter=500)#LogisticRegression(C=0.001, max_iter=150)\n",
    "    model.fit(train_X, train_Y)\n",
    "    pred_Y = model.predict(test_X)\n",
    "\n",
    "\n",
    "    accuracy = metrics.accuracy_score(test_Y, pred_Y)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_Y, pred_Y)\n",
    "    classification_report = metrics.classification_report(test_Y, pred_Y)\n",
    "\n",
    "    print(accuracy) #, print(confusion_matrix), print(classification_report)\n",
    "    avg_acc.append(accuracy)\n",
    "\n",
    "print(round(np.mean(avg_acc),2), np.std(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "76638d9d-d320-45b6-9bc1-9f1aa80dab3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['P20_R', 'P54_N', 'P38_A', ..., 'P8_R', 'P28_A', 'P38_R'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pid[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fa0b87da-c03d-4891-8a90-6d4de79e1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv(output_path.format(sv) + \"x_train_{}.csv\".format(data_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d11ac887-5d5c-4d24-8353-e793c095c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pid = train_pid[:, 0]\n",
    "temp_pid = [i.split(\"_\")[0] for i in temp_pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "89e47b6f-f200-4b5d-984d-0e2f8d68a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_X)\n",
    "train_df[\"pid\"] = temp_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ab9c831a-ffc7-4c0b-a48b-7e5848aa8a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>pid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894999</td>\n",
       "      <td>0.559688</td>\n",
       "      <td>-0.690173</td>\n",
       "      <td>0.831021</td>\n",
       "      <td>0.559537</td>\n",
       "      <td>1.100548</td>\n",
       "      <td>0.065464</td>\n",
       "      <td>0.427734</td>\n",
       "      <td>0.218339</td>\n",
       "      <td>-0.570688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446356</td>\n",
       "      <td>-0.059032</td>\n",
       "      <td>-1.072284</td>\n",
       "      <td>-0.213050</td>\n",
       "      <td>-0.026412</td>\n",
       "      <td>0.969252</td>\n",
       "      <td>1.814752</td>\n",
       "      <td>-1.871549</td>\n",
       "      <td>2.551862</td>\n",
       "      <td>P20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.651896</td>\n",
       "      <td>-1.117284</td>\n",
       "      <td>-0.441646</td>\n",
       "      <td>-0.050600</td>\n",
       "      <td>-0.779768</td>\n",
       "      <td>0.913652</td>\n",
       "      <td>0.795446</td>\n",
       "      <td>-1.974349</td>\n",
       "      <td>0.743754</td>\n",
       "      <td>-0.082648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534210</td>\n",
       "      <td>1.103280</td>\n",
       "      <td>0.668201</td>\n",
       "      <td>-0.409789</td>\n",
       "      <td>0.273002</td>\n",
       "      <td>-0.827151</td>\n",
       "      <td>-0.962456</td>\n",
       "      <td>0.687372</td>\n",
       "      <td>-0.413999</td>\n",
       "      <td>P54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.051238</td>\n",
       "      <td>0.426738</td>\n",
       "      <td>-0.469260</td>\n",
       "      <td>-1.238088</td>\n",
       "      <td>-0.658013</td>\n",
       "      <td>1.118069</td>\n",
       "      <td>0.795446</td>\n",
       "      <td>0.599311</td>\n",
       "      <td>-1.066010</td>\n",
       "      <td>-0.204658</td>\n",
       "      <td>...</td>\n",
       "      <td>1.793923</td>\n",
       "      <td>0.788091</td>\n",
       "      <td>-0.064635</td>\n",
       "      <td>-0.311419</td>\n",
       "      <td>-1.643249</td>\n",
       "      <td>0.071051</td>\n",
       "      <td>0.091768</td>\n",
       "      <td>0.687372</td>\n",
       "      <td>-0.413999</td>\n",
       "      <td>P38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.073776</td>\n",
       "      <td>0.234379</td>\n",
       "      <td>-0.093707</td>\n",
       "      <td>-0.638346</td>\n",
       "      <td>-1.266788</td>\n",
       "      <td>-1.072113</td>\n",
       "      <td>0.430455</td>\n",
       "      <td>0.599311</td>\n",
       "      <td>-0.482215</td>\n",
       "      <td>0.893433</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446356</td>\n",
       "      <td>-1.018857</td>\n",
       "      <td>0.759806</td>\n",
       "      <td>-0.114681</td>\n",
       "      <td>-0.445592</td>\n",
       "      <td>1.025390</td>\n",
       "      <td>1.808243</td>\n",
       "      <td>0.782146</td>\n",
       "      <td>-0.413999</td>\n",
       "      <td>P16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.089389</td>\n",
       "      <td>-0.711419</td>\n",
       "      <td>-0.538295</td>\n",
       "      <td>0.145316</td>\n",
       "      <td>-2.119073</td>\n",
       "      <td>-1.294051</td>\n",
       "      <td>-2.124483</td>\n",
       "      <td>0.170368</td>\n",
       "      <td>1.210790</td>\n",
       "      <td>-0.936718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534210</td>\n",
       "      <td>-1.188977</td>\n",
       "      <td>-0.705866</td>\n",
       "      <td>-0.311419</td>\n",
       "      <td>1.470659</td>\n",
       "      <td>-1.332389</td>\n",
       "      <td>-0.962456</td>\n",
       "      <td>-0.355152</td>\n",
       "      <td>-0.413999</td>\n",
       "      <td>P30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 347 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.894999  0.559688 -0.690173  0.831021  0.559537  1.100548  0.065464   \n",
       "1 -0.651896 -1.117284 -0.441646 -0.050600 -0.779768  0.913652  0.795446   \n",
       "2  0.051238  0.426738 -0.469260 -1.238088 -0.658013  1.118069  0.795446   \n",
       "3 -1.073776  0.234379 -0.093707 -0.638346 -1.266788 -1.072113  0.430455   \n",
       "4 -0.089389 -0.711419 -0.538295  0.145316 -2.119073 -1.294051 -2.124483   \n",
       "\n",
       "          7         8         9  ...       337       338       339       340  \\\n",
       "0  0.427734  0.218339 -0.570688  ... -0.446356 -0.059032 -1.072284 -0.213050   \n",
       "1 -1.974349  0.743754 -0.082648  ... -0.534210  1.103280  0.668201 -0.409789   \n",
       "2  0.599311 -1.066010 -0.204658  ...  1.793923  0.788091 -0.064635 -0.311419   \n",
       "3  0.599311 -0.482215  0.893433  ... -0.446356 -1.018857  0.759806 -0.114681   \n",
       "4  0.170368  1.210790 -0.936718  ... -0.534210 -1.188977 -0.705866 -0.311419   \n",
       "\n",
       "        341       342       343       344       345  pid  \n",
       "0 -0.026412  0.969252  1.814752 -1.871549  2.551862  P20  \n",
       "1  0.273002 -0.827151 -0.962456  0.687372 -0.413999  P54  \n",
       "2 -1.643249  0.071051  0.091768  0.687372 -0.413999  P38  \n",
       "3 -0.445592  1.025390  1.808243  0.782146 -0.413999  P16  \n",
       "4  1.470659 -1.332389 -0.962456 -0.355152 -0.413999  P30  \n",
       "\n",
       "[5 rows x 347 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbd6ac-c0e9-4688-a528-7293b9f3186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"default_noscale\"\n",
    "output_path = '/home/ashish/Results/Datasets/Shimmer/Rowing/Manual/103007/'\n",
    "train_df = pd.read_csv(output_path + \"train_{}.csv\".format(data_type))\n",
    "test_df = pd.read_csv(output_path + \"test_{}.csv\".format(data_type))\n",
    "\n",
    "\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# train_df = shuffle(train_df)\n",
    "# test_df = shuffle(test_df)\n",
    "full_df = shuffle(full_df)\n",
    "\n",
    "print(train_df.shape, test_df.shape, full_df.shape)\n",
    "\n",
    "\n",
    "scores = []\n",
    "accuracy_scores_mapping = {\"pid\": [], \"score\": []}\n",
    "\n",
    "\n",
    "model = LogisticRegression(C=0.001, max_iter=150)\n",
    "\n",
    "unique_pids = list(np.unique(full_df.iloc[:, -2]))\n",
    "\n",
    "for pid in unique_pids:\n",
    "    logger.info(\"The pid: {} \".format(pid))\n",
    "    accuracy_scores_mapping[\"pid\"].append(pid)\n",
    "    train_X, train_Y, test_X, test_Y = generate_cross_val_splits(pid)\n",
    "    logger.info(\"Training shape: {} {}\".format(train_X.shape, train_Y.shape))\n",
    "    logger.info(\"Testing shape: {} {}\".format(test_X.shape, test_Y.shape))\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    train_X = sc.fit_transform(train_X)\n",
    "    test_X = sc.transform(test_X)\n",
    "\n",
    "    model.fit(train_X, train_Y)\n",
    "\n",
    "    pred_Y = model.predict(test_X)\n",
    "    accuracy = metrics.accuracy_score(test_Y, pred_Y)\n",
    "    \n",
    "    scores.append(accuracy)\n",
    "    accuracy_scores_mapping[\"score\"].append(accuracy)\n",
    "    logger.info(\"The accuracy: {} \".format(accuracy))\n",
    "accuracy_scores_mapping_df = pd.DataFrame(accuracy_scores_mapping)\n",
    "accuracy_scores_mapping_df.to_csv(\"/tmp/accuracy_cross_val_shimmer_mp_manual.csv\", index=False)\n",
    "logger.info(\"The average accuracy: {} std: {}\".format(np.mean(scores), np.std(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
